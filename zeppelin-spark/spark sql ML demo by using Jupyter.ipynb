{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build Sparksession and load teh data with CSV format\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"medianHouseValue\", FloatType()),\n",
    "    StructField(\"medianIncome\", FloatType()),\n",
    "    StructField(\"housingMedianAge\", FloatType()),\n",
    "    StructField(\"totalRooms\",FloatType()),\n",
    "    StructField(\"totalBedRooms\",FloatType()),\n",
    "    StructField(\"population\",FloatType()),\n",
    "    StructField(\"households\",FloatType()),\n",
    "    StructField(\"latitude\",FloatType()),\n",
    "    StructField(\"Longitude\",FloatType())\n",
    "])\n",
    "\n",
    "spark=SparkSession.builder.appName(\"Python Spark SQL ML demo\").getOrCreate()\n",
    "\n",
    "df=spark.read.format(\"csv\").schema(schema).option(\"header\", \"true\").load(\"/user/spark/cadata.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- medianHouseValue: float (nullable = true)\n",
      " |-- medianIncome: float (nullable = true)\n",
      " |-- housingMedianAge: float (nullable = true)\n",
      " |-- totalRooms: float (nullable = true)\n",
      " |-- totalBedRooms: float (nullable = true)\n",
      " |-- population: float (nullable = true)\n",
      " |-- households: float (nullable = true)\n",
      " |-- latitude: float (nullable = true)\n",
      " |-- Longitude: float (nullable = true)\n",
      "\n",
      "+----------------+------------+----------------+----------+-------------+----------+----------+--------+---------+\n",
      "|medianHouseValue|medianIncome|housingMedianAge|totalRooms|totalBedRooms|population|households|latitude|Longitude|\n",
      "+----------------+------------+----------------+----------+-------------+----------+----------+--------+---------+\n",
      "|        452600.0|      8.3252|            41.0|     880.0|        129.0|     322.0|     126.0|   37.88|  -122.23|\n",
      "|        358500.0|      8.3014|            21.0|    7099.0|       1106.0|    2401.0|    1138.0|   37.86|  -122.22|\n",
      "|        352100.0|      7.2574|            52.0|    1467.0|        190.0|     496.0|     177.0|   37.85|  -122.24|\n",
      "|        341300.0|      5.6431|            52.0|    1274.0|        235.0|     558.0|     219.0|   37.85|  -122.25|\n",
      "|        342200.0|      3.8462|            52.0|    1627.0|        280.0|     565.0|     259.0|   37.85|  -122.25|\n",
      "|        269700.0|      4.0368|            52.0|     919.0|        213.0|     413.0|     193.0|   37.85|  -122.25|\n",
      "|        299200.0|      3.6591|            52.0|    2535.0|        489.0|    1094.0|     514.0|   37.84|  -122.25|\n",
      "|        241400.0|        3.12|            52.0|    3104.0|        687.0|    1157.0|     647.0|   37.84|  -122.25|\n",
      "|        226700.0|      2.0804|            42.0|    2555.0|        665.0|    1206.0|     595.0|   37.84|  -122.26|\n",
      "|        261100.0|      3.6912|            52.0|    3549.0|        707.0|    1551.0|     714.0|   37.84|  -122.25|\n",
      "+----------------+------------+----------------+----------+-------------+----------+----------+--------+---------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+----------------+--------+\n",
      "|housingMedianAge|count(1)|\n",
      "+----------------+--------+\n",
      "|            52.0|    1273|\n",
      "|            51.0|      48|\n",
      "|            50.0|     136|\n",
      "|            49.0|     134|\n",
      "|            48.0|     177|\n",
      "|            47.0|     198|\n",
      "|            46.0|     245|\n",
      "|            45.0|     294|\n",
      "|            44.0|     356|\n",
      "|            43.0|     353|\n",
      "|            42.0|     368|\n",
      "|            41.0|     296|\n",
      "|            40.0|     304|\n",
      "|            39.0|     369|\n",
      "|            38.0|     394|\n",
      "|            37.0|     537|\n",
      "|            36.0|     862|\n",
      "|            35.0|     824|\n",
      "|            34.0|     689|\n",
      "|            33.0|     615|\n",
      "+----------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Small Data Exploration data analysis\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "df.createOrReplaceTempView(\"dfv\")\n",
    "spark.sql(\"select * from dfv\").show(10)\n",
    "\n",
    "spark.sql(\"select housingMedianAge, count(*) from dfv group by housingMedianAge order by housingMedianAge desc\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+-----------------+------------------+-----------------+-----------------+-------------------+\n",
      "|summary|  medianHouseValue|      medianIncome|  housingMedianAge|        totalRooms|    totalBedRooms|        population|       households|         latitude|          Longitude|\n",
      "+-------+------------------+------------------+------------------+------------------+-----------------+------------------+-----------------+-----------------+-------------------+\n",
      "|  count|             20640|             20640|             20640|             20640|            20640|             20640|            20640|            20640|              20640|\n",
      "|   mean|206855.81690891474|3.8706710030346416|28.639486434108527|2635.7630813953488|537.8980135658915|1425.4767441860465|499.5396802325581|35.63186143109965|-119.56970444871473|\n",
      "| stddev|115395.61587441359|1.8998217183639696| 12.58555761211163|2181.6152515827944| 421.247905943133|  1132.46212176534|382.3297528316098|2.135952380602968|  2.003531742932898|\n",
      "|    min|           14999.0|            0.4999|               1.0|               2.0|              1.0|               3.0|              1.0|            32.54|            -124.35|\n",
      "|    max|          500001.0|           15.0001|              52.0|           39320.0|           6445.0|           35682.0|           6082.0|            41.95|            -114.31|\n",
      "+-------+------------------+------------------+------------------+------------------+-----------------+------------------+-----------------+-----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#get summary of the statistics\n",
    "\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(medianHouseValue=4.526), Row(medianHouseValue=3.585)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Preprocessing( for demo purpose allzero values have been excluded from the data set)\n",
    "\n",
    "#preprocessing the target value ,in this data set dependent variable is medianHouseValue, express hous values in units of 100,000\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "df=df.withColumn(\"medianHouseValue\",col(\"medianHouseValue\")/100000)\n",
    "\n",
    "df.select(\"medianHouseValue\").take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+----------+----------+------------+-----------------+----------------------+-------------------+\n",
      "|medianHouseValue|totalBedRooms|population|households|medianIncome|roomsPerHousehold|populationPerHousehold|    bedroomsPerRoom|\n",
      "+----------------+-------------+----------+----------+------------+-----------------+----------------------+-------------------+\n",
      "|           4.526|        129.0|     322.0|     126.0|      8.3252|6.984126984126984|    2.5555555555555554|0.14659090909090908|\n",
      "|           3.585|       1106.0|    2401.0|    1138.0|      8.3014|6.238137082601054|     2.109841827768014|0.15579659106916466|\n",
      "+----------------+-------------+----------+----------+------------+-----------------+----------------------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Feature Enginering (decide the feaures )\n",
    "\n",
    "#Add some new features\n",
    "df=df.withColumn(\"roomsPerHousehold\", col(\"totalRooms\")/col(\"households\")).\\\n",
    "withColumn(\"populationPerHousehold\",col(\"population\")/col(\"households\")).\\\n",
    "withColumn(\"bedroomsPerRoom\",col(\"totalBedRooms\")/col(\"totalRooms\"))\n",
    "df.first()\n",
    "\n",
    "#Re-order selected columns\n",
    "\n",
    "df.createOrReplaceTempView(\"dfv\")\n",
    "df=spark.sql(\"select medianHouseValue,totalBedRooms,population,households,medianIncome,roomsPerHousehold,populationPerHousehold,bedroomsPerRoom from dfv\")\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=4.526, features=DenseVector([129.0, 322.0, 126.0, 8.3252, 6.9841, 2.5556, 0.1466])),\n",
       " Row(label=3.585, features=DenseVector([1106.0, 2401.0, 1138.0, 8.3014, 6.2381, 2.1098, 0.1558]))]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Separating the features from the target variable\n",
    "\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "\n",
    "indata=df.rdd.map(lambda x: (x[0], DenseVector(x[1:])))\n",
    "df=spark.createDataFrame(indata,[\"label\",\"features\"])\n",
    "df.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=4.526, features=DenseVector([129.0, 322.0, 126.0, 8.3252, 6.9841, 2.5556, 0.1466]), features_scaled=DenseVector([0.3062, 0.2843, 0.3296, 4.3821, 2.8228, 0.2461, 2.5264])),\n",
       " Row(label=3.585, features=DenseVector([1106.0, 2401.0, 1138.0, 8.3014, 6.2381, 2.1098, 0.1558]), features_scaled=DenseVector([2.6255, 2.1202, 2.9765, 4.3696, 2.5213, 0.2031, 2.6851]))]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scale the data by using spark MLlib\n",
    "\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "# Initialize the standardscaler\n",
    "\n",
    "standardScaler = StandardScaler(inputCol=\"features\",outputCol=\"features_scaled\")\n",
    "\n",
    "#fit the DataFrame to the scaler\n",
    "\n",
    "scaler=standardScaler.fit(df)\n",
    "\n",
    "#Transform data in df with the scaler\n",
    "\n",
    "scaleddf = scaler.transform(df)\n",
    "\n",
    "scaleddf.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([0.0, 0.0, 0.0, 0.2796, 0.0, 0.0, 0.0])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building A ML model by using spark MLlib liner regression model\n",
    "\n",
    "train_data,test_data=scaleddf.randomSplit([.8,.2],seed=1234)\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr=LinearRegression(labelCol=\"label\",maxIter=10,regParam=0.3,elasticNetParam=0.8)\n",
    "\n",
    "lm=lr.fit(train_data)\n",
    "\n",
    "predicted=lm.transform(test_data)\n",
    "\n",
    "predictions = predicted.select(\"prediction\").rdd.map(lambda x: x[0])\n",
    "labels =predicted.select(\"label\").rdd.map(lambda x: x[0])\n",
    "\n",
    "predictionandlabel =predictions.zip(labels).collect()\n",
    "\n",
    "predictionandlabel[:5]\n",
    "\n",
    "\n",
    "# evaluate model\n",
    "\n",
    "lm.coefficients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9841344205626824"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8765335684459216"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.summary.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42282227755911483"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.summary.r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
